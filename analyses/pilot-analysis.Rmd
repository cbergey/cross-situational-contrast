---
title: "Compare processing pipelines"
author: "Kevin Kim, Claire Bergey, and Dan Yurovsky"
date: '`r Sys.Date()`'
output:
  html_document:
  highlight: tango
theme: sandstone
code_folding: show
toc: false
toc_float: false
---
  
```{r setup, include=FALSE}
library(tidyverse)
library(jsonlite)
library(here)
library(tidyboot)
library(ggthemes)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = TRUE, tidy = FALSE)

theme_set(theme_few(base_size = 14))
```


```{r read-csv, eval = FALSE}
read_file <- function(file) {

  path = paste("data/", file, sep = "")
  csv_out <- read_csv(here(path))
  id <- str_remove(file, ".csv")
  csv_out$subid <- rep(id, nrow(csv_out))
 
 return(csv_out)
}

file_list <- list.files(path = here("data/"), pattern = "\\.csv$")

raw_data <- map_dfr(file_list, read_file) %>%
  bind_rows() 

#write.csv(data, here("data/pilot-data.csv"))
#data <- read.csv(here("data/pilot-data.csv"))
```

```{r read-data}
raw_data <- read_csv(here("data/pilot-data.csv")) %>%
  select(-X1)
```

```{r prep-data, fig.width = 6, fig.height = 4}
processed_data <- raw_data %>%
  filter(!is.na(phase), phase %in% c("training", "testing")) %>%
  mutate_at(vars(rt, correct), as.numeric) %>%
  mutate(block = floor(trial_index/8),
         size_bin = floor((objSize - 100)/40),
         log_rt = log(rt))

processed_data %>%
  pivot_longer(cols = c(rt, log_rt), names_to = "measure") %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ measure, scales = "free") +
  geom_histogram()

processed_data %>%
  group_by(subid, phase) %>%
  summarise(correct = mean(correct, na.rm = TRUE)) %>%
  ggplot(aes(x = correct)) +
  facet_wrap(~ phase, scales = "free") +
  geom_histogram()
```

Response times are definitely skewed, log transforming them seems right. Cut off rts that are too fast or too slow?

Training/test accuracy also looks super skewed. Also drop people below some threshold?

```{r filtering}
keep_subjs <- processed_data %>%
  group_by(subid) %>%
  summarise(correct = mean(correct)) %>%
  filter(correct >= .7) %>%
  pull(subid)
```

We're keeping `r length(keep_subjs)` out of `r nrow(distinct(processed_data, subid))` original participants

```{r tidy-data}
tidy_data <- processed_data %>%
  filter(subid %in% keep_subjs) %>%
  filter(rt >= 100 && rt <= 3000)
```

```{r accuracy-train,  fig.width = 10, fig.height = 4}
training_accuracy <- tidy_data %>%
  filter(phase == "training") %>%
  group_by(block, object, subid) %>%
  summarise_at(vars(correct, log_rt), mean) %>%
  pivot_longer(cols = c(correct, log_rt), names_to = "measure") %>%
  group_by(block, object, measure) %>%
  tidyboot_mean(value)

ggplot(training_accuracy, aes(x = block, y = empirical_stat, 
                                   color = object)) +
  facet_wrap(~ measure, scales = "free") + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_line() +  
  scale_color_ptol()
```


Accuracy gets better, response times get faster.

```{r accuracy-train-bin-block,  fig.width = 10, fig.height = 4}
training_bin_accuracy <- tidy_data %>%
  filter(phase == "training") %>%
  group_by(block, object, size_bin, subid) %>%
  summarise_at(vars(correct, log_rt), mean) %>%
  pivot_longer(cols = c(correct, log_rt), names_to = "measure") %>%
  group_by(block, object, size_bin, measure) %>%
  tidyboot_mean(value)

ggplot(training_bin_accuracy, aes(x = size_bin, y = empirical_stat, 
                                   color = object)) +
  facet_grid(measure ~ block, scales = "free") + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_line() +
  scale_color_ptol()
```

```{r accuracy-train-bin, fig.width = 10, fig.height = 4}
training_bin_overall_accuracy <- tidy_data %>%
  filter(phase == "training") %>%
  group_by(object, size_bin, subid) %>%
  summarise_at(vars(correct, log_rt), mean) %>%
  pivot_longer(cols = c(correct, log_rt), names_to = "measure") %>%
  group_by(object, size_bin, measure) %>%
  tidyboot_mean(value)

ggplot(training_bin_overall_accuracy, aes(x = size_bin, y = empirical_stat, 
                                   color = object)) +
  facet_wrap(~ measure, scales = "free") + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.5)) + 
  geom_line() + 
  scale_color_ptol()
```

Some hint of an effect where the big object is faster/more accurate for big sizes and small is faster/more accurate for small sizes? but super noisy

```{r accuracy-test-bin-block,  fig.width = 10, fig.height = 4}
test_bin_accuracy <- tidy_data %>%
  filter(phase == "testing") %>%
  group_by(object, size_bin, subid) %>%
  summarise_at(vars(correct, log_rt), mean) %>%
  pivot_longer(cols = c(correct, log_rt), names_to = "measure") %>%
  group_by(object, size_bin, measure) %>%
  tidyboot_mean(value)

ggplot(test_bin_accuracy, aes(x = size_bin, y = empirical_stat, 
                                   color = object)) +
  facet_wrap(~ measure, scales = "free") + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.5)) + 
  geom_line() +
  scale_color_ptol()
```

Unclear what's happening. Maybe some hint of the accuracy effect in that super restricted range? Try subsetting to just correct trials

```{r accuracy-test, fig.width = 6, fig.height = 4}
test_bin_rt <- tidy_data %>%
  filter(phase == "testing", correct == 1) %>%
  group_by(object, size_bin, subid) %>%
  summarise(log_rt = mean(log_rt)) %>%
  tidyboot_mean(log_rt)

ggplot(test_bin_rt, aes(x = size_bin, y = empirical_stat, 
                                   color = object)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.5)) + 
  geom_line() + 
  labs(y = "log response time") +
  scale_color_ptol()
```
